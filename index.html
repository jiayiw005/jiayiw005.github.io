<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <title>Home</title>
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="icon" href="/favicon.ico" type="image/x-icon">


  <style>
    body {
        max-width: 101ch;
        margin: 2em auto;
    }

    .sidebox {
        float: right;
        width: 205px;
        margin-left: 12px;
        margin-bottom: 8px;
        font-size: 0.95em;
    }

    .sidebox p {
        margin: 0.4em 0;
    }

    .intro::after {
        content: "";
        display: block;
        clear: both;
    }

</style>
</head>

<body>



<div class="intro">

    

    <div class="sidebox">
    <img src="images/headshot.png" alt="Picture of Jiayi Wu" width="205">

    <p><strong>Jiayi Wu</strong> <i>(jyah-yee/jiƒÅ-y√¨)</i></p>
    <p>
        <a href="mailto:jiayi_wu4@brown.edu">Email</a> --
        <a href="https://scholar.google.com/citations?user=Rv53IzEAAAAJ&hl=en">Google Scholar</a> --
        <a href="cv.pdf">CV</a>
    </p>
    <p>
        <a href="https://github.com/jiayiw005">GitHub</a> --
        <a href="https://www.linkedin.com/in/jiayiw005/">LinkedIn</a> --
        <a href="news.html">Updates</a>
    </p>
    </div>

    <h1>Hello:)</h1>
    <p>
    Hi! I'm an undergraduate studying Mathematics-Computer Science (Sc.B.) and Applied Mathematics (A.B.) at Brown University.
    </p>


    <p>
    I hope to build the <em>formal</em> and <em>empirical</em> infrastructure for
    <strong>trustworthy machine learning</strong> across two complementary
    dimensions:
    </p>

    <ul>
    <li>
        on the <em>learning</em> side, model architectures informed by structured latent
        representations and neurosymbolic approaches <sup><a href="cv.pdf#lean4-proof-mining">[1]</a>,
        <a href="cv.pdf#mot">[2]</a></sup>, and how they support more robust and interpretable reasoning models.
    </li>
    <li>
        on the <em>guarantee</em> side, evaluation and auditing pipeline design
        <sup><a href="cv.pdf#overton-evals">[3]</a>, <a href="cv.pdf#am-auditing">[4]</a>
        </sup>
        informed by formal verification
        <sup><a href="cv.pdf#pluggable-analysis">[5]</a>, <a href="cv.pdf#lean4-proof-mining">[1]</a></sup>,
        and how they scaffold more aligned and accountable automated systems.
    </li>
    </ul>
    <p>In particular, I'm interested in formal theorem proving and program synthesis as fully specified, unambiguous, verifiable substrates for studying models' reasoning processes. </p>
</div>

<h3>My Work</h3>

<p>
  Below are selected projects and publications; a full list of research projects is available in my <a href="cv.pdf">academic CV</a>.

<ul>
  <li>
    Pattern Mining and Automated Tactic Discovery in Lean4 Theorem Proving, in collaboration with Gavin Zhao, 
    advised by
    <a href="https://robertylewis.com/">Robert Lewis</a>
    and
    <a href="https://cs.brown.edu/people/sbach/">Stephen Bach</a>
  </li>

  <li>
    Formalizing and Benchmarking Overton Pluralism in LLMs
    <a href="https://arxiv.org/pdf/2512.01351">[Preprint]</a>,
    in collaboration with
    <a href="https://elinorp-d.github.io/">Elinor Poole-Dayan</a>
    <em>et al.</em>,
    advised by
    <a href="https://miba.dev/">Michiel Bakker</a>,
    accepted to NeurIPS 2025 Workshop on
    <a href="https://sites.google.com/view/llm-eval-workshop/">
      Evaluating the Evolving LLM Lifecycle: Benchmarks, Emergent Abilities, and Scaling
    </a>
  </li>

  <li>
    Motif of Thoughts (MoT): Reusable Abstractions for Compositional Neurosymbolic Reasoning,
    in collaboration with
    <a href="https://cjj.li/research/">Chance Jiajie Li</a>
    <em>et al.</em>

    <ul>
      <li>
        Simulating Society Requires Simulating Thought
        (position paper on causally grounded reasoning modules for multi-agent systems)
        <a href="https://www.arxiv.org/pdf/2506.06958">[Preprint]</a>,
        accepted to The Thirty-ninth Annual Conference on Neural Information Processing Systems (NeurIPS 2025)
      </li>

      <li>
        HugAgent: Evaluating LLMs in Simulating Individual Reasoning on Open-Ended Tasks
        <a href="https://arxiv.org/pdf/2510.15144">[Preprint]</a>
        <a href="https://anonymous.4open.science/r/HugAgent/">[Repository]</a>,
        accepted to NeurIPS 2025 Workshop on
        <a href="https://sites.google.com/view/law-2025">
          LAW 2025: Bridging Language, Agent, and World Models
        </a>
        <em style="color:red">(Spotlight)</em>, 
        <a href="https://personallmworkshop.github.io/">PersonaLLM: Workshop on LLM Persona Modeling</a>
        <em style="color:red">(Oral)</em>, and <a href="https://responsible-fm.github.io/">Socially Responsible and Trustworthy Foundation Models (ResponsibleFM)</a>
      </li>
    </ul>
  </li>

  <li>
    Pluggable Analyses for Modern Real Systems, 
    <a href="https://utra.brown.edu/">Undergraduate Teaching and Research Awards (UTRA)</a> project advised by
    <a href="https://nikos.vasilak.is/">Nikos Vasilakis</a>
  </li>

  <li>
    Algorithmic Fairness and AI Governance Primer at Socially Responsible Computing (SRC) Handbook
    <a href="https://srch.cs.brown.edu">[Handbook]</a>,
    advised by
    <a href="https://dsi.brown.edu/people/suresh-venkatasubramanian">
      Suresh Venkatasubramanian
    </a>
    and
    <a href="http://www.julianetter.de/">Julia Netter</a>
  </li>
</ul>



<h3>Misc</h3>
<p>Outside of research, I‚Äôm co-president of Brown‚Äôs <a href="https://www.theaires.org/">AI and Robotics Ethics Society (AIRES)</a>; 
  I also co-directed the AI Governance Panel at <a href="https://www.brownchinasummit.org/">Brown China Summit 2025</a>; 
  more updates are available <a href="news.html">here</a>!</p>
<p>
  In my spare time, I enjoy cold brew, taking on plank challenges (‚â•7min),
  and roaming around the neighborhoods/community orgs where I live and volunteer
  üèòÔ∏èüå≥ -- if I'm not in
  <a href="https://cs.brown.edu/about/directions/">CIT</a>
  :)
</p>

</body>
</html>